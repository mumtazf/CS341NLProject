{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops = pd.read_csv(\"data/laptops.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "features_dict['display_size'] = [10.1, 11.6, 12.4, 13.0, 13.3, 13.4, 13.5, 13.6, 14.0, 14.1, 14.2, 14.5, 15.0, 15.3, 15.6, 16.0, 16.1, 16.2, 17.3, 18.0]\n",
    "features_dict['brand'] = ['acer', 'apple', 'asus', 'avita', 'axl', 'chuwi', 'dell', 'fujitsu', 'gigabyte', 'honor', 'hp', 'iball', 'infinix', 'jio', 'lenovo', 'lg', 'microsoft', 'msi', 'primebook', 'realme', 'samsung', 'tecno', 'ultimus', 'walker', 'wings', 'zebronics']\n",
    "features_dict['ram_memory'] = ['2gb', '4gb', '8gb', '12gb', '16gb', '32gb', '36gb']\n",
    "features_dict['processor_tier'] = ['celeron', 'core i3', 'core i5', 'core i7', 'core i9', 'core ultra 7', 'm1', 'm2', 'm3', 'other', 'pentium', 'ryzen 3', 'ryzen 5', 'ryzen 7', 'ryzen 9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "new_corpus = api.load(\"word2vec-google-news-300\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(model.vector_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_embeddings = {}\n",
    "for category, items in features_dict.items():\n",
    "    precomputed_embeddings[category] = {\n",
    "        item: get_embedding(str(item), new_corpus) for item in items\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(input_vector, category_embeddings):\n",
    "    similarities = {}\n",
    "    for label, vector in category_embeddings.items():\n",
    "        if np.any(vector):\n",
    "            similarity = cosine_similarity([input_vector], [vector])[0][0]\n",
    "            similarities[label] = similarity\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tokens(tokens, precomputed_embeddings, model):\n",
    "    results = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        embedding = get_embedding(token, model)\n",
    "        \n",
    "        if not np.any(embedding): # if there is no embedding returned, then we return 0\n",
    "            results.append((token, \"unknown\", 0))\n",
    "            continue\n",
    "        \n",
    "        # Calculate similarity for each category\n",
    "        token_scores = {}\n",
    "        for category, embeddings in precomputed_embeddings.items():\n",
    "            similarities = calculate_cosine_similarity(embedding, embeddings)\n",
    "            if similarities:\n",
    "                best_match = max(similarities, key=similarities.get)\n",
    "                token_scores[category] = (best_match, similarities[best_match])\n",
    "        \n",
    "        # Find the highest similarity category\n",
    "        if token_scores:\n",
    "            final_category = max(token_scores, key=lambda x: token_scores[x][1])\n",
    "            best_label, best_score = token_scores[final_category]\n",
    "\n",
    "            if best_score > 0.7: ## our score is 0.5 for it to be classified as a label\n",
    "                results.append((token, final_category, best_score))\n",
    "        else:\n",
    "            results.append((token, \"unknown\", 0))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(input):\n",
    "    tokens = input.split(\" \")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    return [token for token in tokens if token not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token is dell, it's score is 1.0000001192092896, and the category is brand\n",
      "token is please., it's score is 0, and the category is unknown\n",
      "token is 1500., it's score is 0, and the category is unknown\n",
      "token is 8gb, it's score is 1.0, and the category is ram_memory\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "input = \"i want a core i9 laptop from dell brand please. my budget is 1500. it should be 8gb\"\n",
    "tokens = preprocess_sentences(input)\n",
    "\n",
    "result = classify_tokens(tokens, precomputed_embeddings,new_corpus)\n",
    "\n",
    "for token, category, score in result: \n",
    "    print(f\"token is {token}, it's score is {score}, and the category is {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "domain_sentences = ['celeron', 'core i3', 'core i5', 'core i7', 'core i9', 'core ultra 7', 'm1', 'm2', 'm3', 'other', 'pentium', 'ryzen 3', 'ryzen 5', 'ryzen 7', 'ryzen 9']\n",
    "# Assuming you have a FastText model trained or loaded\n",
    "fasttext_model = FastText(sentences=domain_sentences, vector_size=300)\n",
    "\n",
    "embedding = fasttext_model.wv['core i7']  # FastText handles phrases better \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
